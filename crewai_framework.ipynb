{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e90a8d68-9a9a-4b0e-a824-530cd474b3e8",
   "metadata": {},
   "source": [
    "# 1. Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9bbe04-b3c5-419d-8159-eb2b4b67d83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\mouda\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mouda\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\mouda\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mouda\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib seaborn\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from crewai import Agent,Task ,Crew, Process\n",
    "from dotenv import load_dotenv\n",
    "from crewai_tools import PDFSearchTool\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0219a-8293-4109-897d-96fbf4f9d3ac",
   "metadata": {},
   "source": [
    "# 2.  DATA Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a067739-fccc-4571-b49d-b1b6be4b2a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11552</td>\n",
       "      <td>They are more likely to die of coronavirus, we...</td>\n",
       "      <td>nothate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11846</td>\n",
       "      <td>I endorse the attacking of muslims</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7330</td>\n",
       "      <td>I'm sorry to say that I'm prejudiced against p...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1802</td>\n",
       "      <td>Hijabis are shit</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16497</td>\n",
       "      <td>The savages that commit these heinous acts do ...</td>\n",
       "      <td>nothate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>6013</td>\n",
       "      <td>Being a Jew I am a treated like a dirty rat</td>\n",
       "      <td>nothate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>27958</td>\n",
       "      <td>I don't see myself being friends with people w...</td>\n",
       "      <td>nothate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>617</td>\n",
       "      <td>sarah do you realise that we live in australia...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>5146</td>\n",
       "      <td>nigger is illiterate and can't even read or wr...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>20428</td>\n",
       "      <td>Child Empowerment International is useless and...</td>\n",
       "      <td>nothate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     example_id                                           sentence    label\n",
       "0         11552  They are more likely to die of coronavirus, we...  nothate\n",
       "1         11846                 I endorse the attacking of muslims     hate\n",
       "2          7330  I'm sorry to say that I'm prejudiced against p...     hate\n",
       "3          1802                                   Hijabis are shit     hate\n",
       "4         16497  The savages that commit these heinous acts do ...  nothate\n",
       "..          ...                                                ...      ...\n",
       "345        6013        Being a Jew I am a treated like a dirty rat  nothate\n",
       "346       27958  I don't see myself being friends with people w...  nothate\n",
       "347         617  sarah do you realise that we live in australia...     hate\n",
       "348        5146  nigger is illiterate and can't even read or wr...     hate\n",
       "349       20428  Child Empowerment International is useless and...  nothate\n",
       "\n",
       "[350 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step1 : Reading HateModerate files \n",
    "hate_df = pd.read_csv('all_examples_hate.csv', delimiter='\\t')  \n",
    "nonhate_df = pd.read_csv('all_examples_nonhate.csv', delimiter='\\t')  \n",
    "# setp2 : Randomly select rows\n",
    "n_rows= 175 # select the sample size you want from each file\n",
    "sampled1_df = hate_df.sample(n=n_rows, random_state=1)  # random_state for reproducibility\n",
    "sampled2_df = nonhate_df.sample(n=n_rows, random_state=1)  # random_state for reproducibility\n",
    "# sep 3:  Select the desired columns\n",
    "result_hate_df = sampled1_df[['example_id', 'sentence']].copy()\n",
    "result_nonhate_df = sampled2_df[['example_id', 'sentence']].copy()\n",
    "\n",
    "#step 4 :  Add a new column with the label 'hate'\n",
    "result_hate_df['label'] = 'hate'\n",
    "result_nonhate_df['label'] = 'nothate'\n",
    "\n",
    "# step 5: combin hate and nothate tables in one table\n",
    "combined_df = pd.concat([result_nonhate_df, result_hate_df], ignore_index=True)\n",
    "\n",
    "# step 6 :mix the rows \n",
    "shuffled_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "## step 7: renaming the final Dataset##\n",
    "processed_hate_df = shuffled_df \n",
    "\n",
    "## step 8 : saving the processed data to a CSV file\n",
    "processed_hate.to_csv('HateModerate_sample.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8849a-0d7a-4feb-ad18-29bd6b22a5d1",
   "metadata": {},
   "source": [
    "# 3. Framwork Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43708caf-46c1-4b8c-9b2c-01161ffd937b",
   "metadata": {},
   "source": [
    "## 3.1 Setting up LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36475440-9f41-41d8-b98d-bf4d63ab6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: openai api key#\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY \n",
    "#step2: setting up the LLMs that will be used \n",
    "\n",
    "llm_gpt4o_mini=ChatOpenAI(model_name=\"gpt-4o-2024-05-13\", temperature=0.0)\n",
    "llm_gpt4o=ChatOpenAI(model_name=\"gpt-4o\", temperature=0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1cfb42-2c50-46cc-8565-5779d82e629d",
   "metadata": {},
   "source": [
    "## 3.2 Designing the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "610919bb-3bba-43c6-81ab-35da2a03bb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "## designing RAG PDFsearching tools ## \n",
    "group_search = PDFSearchTool('Hate attack types.pdf') # replace with protection level file path\n",
    "Attacks_search = PDFSearchTool('Hate Slur.pdf') # replace with Hate attack  file path\n",
    "Slur_search=PDFSearchTool('protection level.pdf') # replace with Hate slur file path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babbdbc6-c053-47cc-9d01-8be2ae8b6021",
   "metadata": {},
   "source": [
    "## 3.3 Designing the four Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7c46a6a-851f-460e-8624-98aa1a690bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "protection_level_agent=Agent(\n",
    "    role='English content analyst',\n",
    "    goal='reading and analyzing {content} and answering questions, passing answers to content moderator',\n",
    "    memory=False,\n",
    "    backstory=(\n",
    "       \"\"\" you are an experienced content analyst; you can access protection level documents using the tool,\n",
    "       Your expertise lies in identifying which groups or individuals are protected and what protection level:\n",
    "         - fully protected, we called them protected characteristics (PCs).\n",
    "         - Partially protected, we called them quasi-protected characteristics (Q-PCs).\n",
    "         - non-protected groups. \n",
    "       \n",
    "       \"\"\" \n",
    "    ),\n",
    "    verbose=True,\n",
    "    max_iter=25,\n",
    "    allow_delegation=False,\n",
    "    max_retry_limit=3,\n",
    "    llm= llm_gpt4o, # <== disable this line if hierarchical is used \n",
    "    tools=[group_search]\n",
    ")\n",
    "\n",
    "attack_type_agent =Agent(\n",
    "    role='English Meta content analyst',\n",
    "    goal='  reading and analyzing {content} and answering questions, passing answers to content moderator',\n",
    "    memory=False,\n",
    "    backstory=(\n",
    "       \"\"\" you are an experienced content analyst working for Meta; you can access hate attack types document using the tool,\n",
    "       Your expertise lies in identifying Hate attacks:\n",
    "         - tier1 \n",
    "         - tier2 \n",
    "         - No Attack\n",
    "       you also can understand the content and see if: \n",
    "         - at least half the group PC or Q-PC were targeted in attacks. \n",
    "         - less than half were targeted \n",
    "       \"\"\" \n",
    "    ),\n",
    "    max_iter=25,\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    max_retry_limit=3,\n",
    "    llm= llm_gpt4o, # <== disable this line if hierarchical is used \n",
    "    tools=[Attacks_search]\n",
    ")\n",
    "\n",
    "Hate_slur_agent =Agent(\n",
    "    role='English content analyst',\n",
    "    goal='reading and analyzing {content} and answering questions, passing answers to content moderator',\n",
    "    memory=False,\n",
    "    backstory=(\n",
    "       \"\"\" you are an experienced content analyst; you can access hate Slur documents using the tool,\n",
    "       Your expertise lies in identifying: \n",
    "       - the hate slurs used\n",
    "       - hate slur exceptions, if there are any. \n",
    "       \"\"\" \n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    max_iter=25,\n",
    "    max_retry_limit=3,\n",
    "    llm= llm_gpt4o, # <== disable this line if hierarchical is used \n",
    "    tools=[Slur_search]\n",
    ")\n",
    "decision_making_agent = Agent(\n",
    "  role=' Content Moderator',\n",
    "  goal='Determine if the content contains hate speech based on predefined scenarios and provides a reason',\n",
    "  backstory=\"\"\"You are an experienced Content moderator agent, and you have a very good understanding of hate speech scenarios.\n",
    "                 Your expertise lies in the following: \n",
    "                 - content labeling \n",
    "                 - identifying the reasons you \n",
    "                 \"\"\",\n",
    "  verbose=True,\n",
    "  allow_delegation=False,\n",
    "  max_retry_limit=3,\n",
    "  memory=False,\n",
    "  max_iter=30,\n",
    "  llm= llm_gpt4o # <== disable this line if hierarchical is used \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a5ab9e-efe8-46c5-9ce5-c269a6fc5211",
   "metadata": {},
   "source": [
    "## 3.4 Designing the Taskes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375711e3-7352-4eea-8f81-af07fb4e61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "defining_group_task = Task(\n",
    "  description=\"\"\" read the content {content}, and answer the following questions, pass the answers to content_modertor\n",
    "  Q1- think step by step, Are any groups of people or Human individuals mentioned? \n",
    "  note: concepts, institutions, or religions are not considered groups of people or Human individuals   \n",
    "   for example: \n",
    "  *Muslims: are a group, thus Yes group\n",
    "  *Islam: is a religion, thus No group.  \n",
    "  *Jewish: is an individual, Yes, Individual. \n",
    "  Q2- think step by step, use the group_search tool, Do the group or individuals mentioned have: \n",
    "  - protected characteristics (PC)  \n",
    "  - protected from most severe attacks, quasi-protected Characteristics (Q-PC)\n",
    "  - None.   \n",
    "   Examples: \n",
    "  *Germans: national origin, thus PC\n",
    "  *13 years old boys: PC age, thus PC\n",
    "  *Syrian Refugees: Syrians are of national origin thus, a PC + Refugees describes immigration status. \n",
    "                     thus, Syrian Refugees are Q-PC, (PC+Q-PC=Q-PC)\n",
    "  \n",
    "  \"\"\",\n",
    "  expected_output=\" Yes or No group | PC or Q-PC or None\",\n",
    "  agent=protection_level_agent\n",
    ")\n",
    "\n",
    "defining_attack_task = Task(\n",
    "  description=\"\"\"read the content {content}, and answer the following questions,  pass the answers to content_modertor\n",
    "  \n",
    "  Q1-think step by step; use the Attacks_search tool to answer this question: Have you found any of tier 1 or tier 2 attacks :  \n",
    "  -  tire 1, define it\n",
    "  -  tier 2, define it\n",
    "  -  No Attack. \n",
    "    \n",
    "  Q2-Think step by step, using only your common sense. Has the attack targeted at least half the group? \n",
    "  - less than half \n",
    "  - half and more \n",
    "   examples: \n",
    "  \"Berliner women are animals\": Berliner women are less than half of women in the world; therefore,\n",
    "  the answer is the attack targeted at less than half.\n",
    "  \"Majority of Russians are not smart\": The majority of Russians are more than half of Russians- PC; \n",
    "  therefore, the answer would be more than half.\n",
    "  \"many Indians do not wash their hands\": many Indians are more than half of them.\n",
    "  \"\"\",\n",
    "  expected_output=\" Tier 1 or tier2 or No Attack | less than half or half and more\",\n",
    "  agent=attack_type_agent \n",
    ")\n",
    "\n",
    "\n",
    "defining_Slur_use_task = Task(\n",
    "  description=\"\"\"\" read the content {content} use the tool to access Slur defintion,and answer the following questions,  \n",
    "  pass the answers to content_modertor \n",
    " \n",
    "  Q1- think step by step, were any Hate slur used?   \n",
    "  -Hate slur was used; name the slur. \n",
    "  -No Hate slur was used\n",
    "  Note: Hate slurs including like Ethnic slur terms (“nigger”, “kike”, “kraut”) and other group-based slurs (“faggot”, “spaz”)\n",
    "  must be differentiated from general pejoratives (“asshole”, “idiot”,\"mother fucker\") and pure expressives (“fuck”) which are not Hate slurs\n",
    "   Examples: \n",
    "    Hate Slurs: Faggot, fag, Nigger, Paki, whitey, nigga .. etc. \n",
    "  Q2- think step by step, Was the hate slur used to condemn the use of the slur, report it, self-referential or in an empowering way\n",
    "  - Yes, Hate slur exception.  \n",
    "  - NO Hate slur exception.  \n",
    "  - No Hate slur was used.  \n",
    "  \"\"\",\n",
    "  expected_output=\" Yes or No Hate slur if it was used |  Yes or No, Hate slur exception\",\n",
    "  agent=Hate_slur_agent\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "task_labeling = Task(\n",
    "  description=\"\"\" read the answers provided, label each {content} as hate or not hate, and provide the reason behind your label:\n",
    "\n",
    "  \n",
    "   Consider hate speech only in these three Scenarios if you have : \n",
    "   - protected characteristics (PC) and [tier1 attack or tier2 attack] and more than half was taregeted .  \n",
    "   - or if you have Quasi-protected Characteristics (Q-PC) and [tier 1 attack only] and more than half.  \n",
    "   - or if a Hate slur was used, and NO Hate slur exception.  \n",
    "   the reason should include if there are any : tier attack type / PC, O-PC or No PC /  the slur if it was used and how it was used. \n",
    "\n",
    "\n",
    "  \"\"\",\n",
    "    \n",
    "  expected_output=\"\"\" label | reason 10 words max \"\"\",\n",
    "  \n",
    "  agent=decision_making_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f652ffd-4a13-45c0-9ccd-62f53e20b998",
   "metadata": {},
   "source": [
    "## 3.4 Designing the Crew "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b754ddf-bd62-4d75-b668-afe06cfd2961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 14:24:28,366 - 29840 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n",
      "2024-09-24 14:24:28,375 - 29840 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "## designing sequential_crew\n",
    "crew_sequential = Crew(\n",
    "  agents=[protection_level_agent,attack_type_agent,Hate_slur_agent,decision_making_agent],\n",
    "  tasks=[defining_group_task,defining_attack_task,defining_Slur_use_task,task_labeling],\n",
    "  process=Process.sequential, \n",
    "  full_output=False,\n",
    "\n",
    "  verbose=2 # You can set it to 1 or 2 to different logging levels \n",
    ")\n",
    "\n",
    "## designing hierarchical crew\n",
    "crew_hierarchical = Crew(\n",
    "  agents=[protection_level_agent,attack_type_agent,Hate_slur_agent,decision_making_agent],\n",
    "  tasks=[defining_group_task,defining_attack_task,defining_Slur_use_task,task_labeling],\n",
    "  process=Process.hierarchical, \n",
    "  manager_llm= llm_gpt4o_mini , #chose the LLM you want to use. \n",
    "  full_output=False,\n",
    "\n",
    "  verbose=2 # You can set it to 1 or 2 to different logging levels \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "babe755b-7e79-4bf3-91bd-8fbd84e4c1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11552</td>\n",
       "      <td>They are more likely to die of coronavirus, we...</td>\n",
       "      <td>nothate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11846</td>\n",
       "      <td>I endorse the attacking of muslims</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7330</td>\n",
       "      <td>I'm sorry to say that I'm prejudiced against p...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_id                                           sentence    label\n",
       "0       11552  They are more likely to die of coronavirus, we...  nothate\n",
       "1       11846                 I endorse the attacking of muslims     hate\n",
       "2        7330  I'm sorry to say that I'm prejudiced against p...     hate"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67a6e575-8576-4431-81d1-639fb2f532cb",
   "metadata": {},
   "source": [
    "# 4 Deployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c096fd-a6a3-4104-90b7-3202f24708d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## looping through dataset & saving results in a list##\n",
    "\n",
    "## step 1: prapearing data for deployment \n",
    "Hate_m_df= pd.read_csv('HateModerate_sample.csv')\n",
    "deplyed_data = processed_hate_df[:10] # here either you can run loop on entire dataframe or slice it like I used only first 10 rows\n",
    "\n",
    "## step 2 : creating empty list \n",
    "label_list = []\n",
    "\n",
    "##step 3: intinlizing the sequential framework for each sentance \n",
    "for i in range(len(deplyed_data)):\n",
    "    result = crew_sequential.kickoff({\"content\": str(deplyed_data [\"sentence\"][i])}) # for hierarchical crew_hierarchical.kickoff\n",
    "    label_list.append(result)\n",
    "\n",
    "##step 4:Adding the labels to the hate_df##\n",
    "deplyed_data[\"new_label\"] = label_list\n",
    "\n",
    "#step 5: saving new data set on separate file  \n",
    "deplyed_data.to_csv('data1.csv', index=False) \n",
    "\n",
    "#step 6 : reading new data set as CSV file  \n",
    "df = pd.read_csv('data1.csv')\n",
    "\n",
    "# step 7:split the 'new_label' into multiple columns new lable and reason\n",
    "df[['new_label', 'resone']] = df['new_label'].str.split('|', expand=True)\n",
    "\n",
    "# step 8:  save the final data ready for evaluation\n",
    "df.to_csv('labeled_data_seq_gpt40.csv', index=False) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f52e6-f349-4c83-8af1-afb263ec39af",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b8da6-16ba-4aa3-8c65-5e2f5cc923d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the combined CSV file\n",
    "df = pd.read_csv('the saved filed in deployment pahse path')  # Replace with your actual file name\n",
    "\n",
    "# Step 2: Convert string labels to binary (1 and 0)\n",
    "df['label'] = df['label'].map({'hate': 1, 'nothate': 0})          # Replace 'label' with actual column name\n",
    "df['new_label'] = df['new_label'].map({'hate': 1, 'nothate': 0})  # Replace 'new label' with actual column name\n",
    "\n",
    "# Step 3: Extract the true labels and predicted labels\n",
    "y_true = df['label']\n",
    "y_pred = df['new_label']\n",
    "\n",
    "# Step 4: Calculate performance metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Step 5: Print the results\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "\n",
    "# Step 6: Calculate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "\n",
    "# Step 7: Visualize the confusion matrix using Seaborn\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Hate', 'Predicted Not Hate'], yticklabels=['Actual Hate', 'Actual Not Hate'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix Hierarchical Process')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
